---
title: "coeffcient_investigation"
author: "Daivd Leslie"
date: "Sunday, April 12, 2015"
output: pdf_document
---

The purpose of the code is to evaluate the effectiveness of the leave one out method for determining the predictive power of each model given. In order to acomplish this, the model given will be trained with data that has been reduced by one sample. The sample that is left out will then be used as a new data point to be predicted by the given model. By looking at the differences in the predicted value by the model and the actual value recorded, a greater insight will be gained as to how well the model preforms when predicting new data. Let's first start by generating a popluation, which we will sample from.

```{r}
# Fix random number generator for consistant data
set.seed(10)

# Population size
pop = 9000

# Create predictor variable(s) with normal distribution
x1 = rnorm(pop)
x2 = rnorm(pop)
x3 = rnorm(pop)

# Organize predictor(s) in data.frame
pop_data = data.frame(x1, x2, x3)

# View data
head(pop_data)

# Set the number of standard deviations to be 
# considered for noise variable
numSds = 8

# Create empty list for noise and responce variable
noise = list()
y = list()

# For loop to generte noise and y values and append them to
# predictor(s) data.frame
for(i in 1:numSds) {
  noise[[i]] = rnorm(pop,0,i)
  y[[i]] = 2*x1 + x2 + 3*x3 + noise[[i]]
  pop_data[, paste('y', i, sep='')] = y[[i]]
}

head(pop_data)

```

Now that we have our population, lets take a sample of our data and verify that the results generated by the model are reasonalbe estimates of the true model's coeffcients (Remebering that they should be close to the exspected values of: x1=2, x2=1, and x3=3).  

```{r}
# Create sample population
n = 375
samp = pop_data[sample(nrow(pop_data), n), ]

# Create model(s)
mod = list()
for(i in 1:numSds) {
  tempStr = paste('y', i, ' ~ x1 + x2 + x3', sep='')
  mod[[i]] = lm(as.formula(tempStr), data=pop_data)
}

lapply(mod, summary)

```

Looking at the model summary, it appears that calculated coeffeicents are fairly close to actual coefficents. It also appears that approximately 60% of the variability can be explained by the predictor variables. This r squared value seems reasonalbe since there is an extra error term incorporated into the model (noise). Now that we feel cofident about the sample taken, lets see how good the models generated will be at predicting new data. In order to do this, lets apply the leave one out method to the data. The two things to note for this method are:

1. How close are the predicted values to the actual vales.

2. How much the coeffcients vary between models

```{r}
# Function(s)
Leave_one_out = function(data, model) {
  matrixCoef = matrix(NA, nrow=nrow(data), ncol=length(coef(model)))
  colnames(matrixCoef) = names(coef(model))
  predicted = NULL
  rSquared = NULL
  absDiff = NULL
  for(i in 1:nrow(data)) {
    trainSet = data[-i,]
    testSet = data[i,]
    trainMod = update(model, data=trainSet)
    matrixCoef[i, ] = coef(trainMod)
    predicted[i] = predict(trainMod, newdata = testSet)
    rSquared[i] = summary(trainMod)$r.squared
    absDiff[i] = abs(testSet$y - predicted[i])
  }
  observed = residuals(mod) + predict(mod)
  finalMatrix = data.frame(matrixCoef, predicted, observed, rSquared, absDiff)
  names(finalMatrix)[1] = 'intercept'
  return(finalMatrix)
}

fit_mod = lm(y ~ x1 + x2 + x3, data=samp)
resultsMatrix = Leave_one_out(samp, fit_mod)

plot(observed ~ predicted, data=resultsMatrix)
abline(a=0, b=1)

head(resultsMatrix)

# Need to fix tame data
hist(samp$y)
samp[375,1] = 40
plot(samp$y,samp$x2)
plot(samp$y,samp$x1)
plot(samp$y,samp$x3)
fit_mod = lm(y ~ x1 + x2 + x3, data=samp)
resultsMatrix = Leave_one_out(samp, fit_mod)
tail(resultsMatrix)

```
From the results, it appears that the coeffeicents are not varying too much, which would suggest that the models are not overfitting the data. However, the differences in the predicted and actual values do vary slightly with each model prediction. Despite this varation, all the models constructed have relatively the same r squared value of roughtly 60%, which would suggest that all of the models have about the same predictive power.  