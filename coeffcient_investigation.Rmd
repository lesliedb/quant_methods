---
title: "coeffcient_investigation"
author: "Daivd Leslie"
date: "Sunday, April 12, 2015"
output: pdf_document
---

The purpose of the code is to ...

```{r}
# Import Statement(s)
library(RandomFields)

# Create Grid
n = 100

# Set distance between points
spat_range = 1:10

# Create spatially autocorralated predictor variables
for(i in spat_range) {
  print(i)
}

# Create  a model with spatial dependence
mod_spat_dep = RMexp(var=1, scale=spat_range)

# Create spatially autocorralated predictor variables
x1 = RFsimulate(mod_spat_dep, x=1:n, y=1:n, grid=T)
x2 = RFsimulate(mod_spat_dep, x=1:n, y=1:n, grid=T)
x3 = RFsimulate(mod_spat_dep, x=1:n, y=1:n, grid=T)

# Convert objects variables to vectors
#spat_err = as.vector(spat_err)
x1 = as.vector(x1)
x2 = as.vector(x2)
x3 = as.vector(x3)

# Organize predictor(s) in data.frame
pop_data = data.frame(x1, x2, x3)

# View data
head(pop_data)

# Set the number of standard deviations to be 
# considered for noise variable
numSds = 8

# Set distance between points (Must be derived from data)
spat_range = 5

# Create spatail error term
spat_err = RFsimulate(mod_spat_dep, x=1:n, y=1:n, grid=T)

# Create empty list for noise and responce variable
noise = list()
y = list()


y = 2*x1 + x2 + 3*x3 + spat_err

# For loop to generte noise and y values and append them to
# predictor(s) data.frame
for(i in 1:numSds) {
  noise[[i]] = rnorm(pop,0,i)
  y[[i]] = 2*x1 + x2 + 3*x3 + noise[[i]]
  pop_data[, paste('y', i, sep='')] = y[[i]]
}

head(pop_data)

```

Now that we have our population, lets take a sample of our data and verify that the results generated by the model are reasonalbe estimates of the true model's coeffcients (Remebering that they should be close to the exspected values of: x1=2, x2=1, and x3=3).  

```{r}
# Create sample population
n = 375
samp = pop_data[sample(nrow(pop_data), n), ]

# Create model(s)
mod = list()
for(i in 1:numSds) {
  tempStr = paste('y', i, ' ~ x1 + x2 + x3', sep='')
  mod[[i]] = lm(as.formula(tempStr), data=pop_data)
}

lapply(mod, summary)

```

Looking at the model summary, it appears that calculated coeffeicents are fairly close to actual coefficents. It also appears that approximately 60% of the variability can be explained by the predictor variables. This r squared value seems reasonalbe since there is an extra error term incorporated into the model (noise). Now that we feel cofident about the sample taken, lets see how good the models generated will be at predicting new data. In order to do this, lets apply the leave one out method to the data. The two things to note for this method are:

1. How close are the predicted values to the actual vales.

2. How much the coeffcients vary between models

```{r}
# Function(s)
Leave_one_out = function(data, model) {
  matrixCoef = matrix(NA, nrow=nrow(data), ncol=length(coef(model)))
  colnames(matrixCoef) = names(coef(model))
  predicted = NULL
  rSquared = NULL
  absDiff = NULL
  for(i in 1:nrow(data)) {
    trainSet = data[-i,]
    testSet = data[i,]
    trainMod = update(model, data=trainSet)
    matrixCoef[i, ] = coef(trainMod)
    predicted[i] = predict(trainMod, newdata = testSet)
    rSquared[i] = summary(trainMod)$r.squared
    absDiff[i] = abs(testSet$y - predicted[i])
  }
  observed = residuals(mod) + predict(mod)
  finalMatrix = data.frame(matrixCoef, predicted, observed, rSquared, absDiff)
  names(finalMatrix)[1] = 'intercept'
  return(finalMatrix)
}

fit_mod = lm(y ~ x1 + x2 + x3, data=samp)
resultsMatrix = Leave_one_out(samp, fit_mod)

plot(observed ~ predicted, data=resultsMatrix)
abline(a=0, b=1)

head(resultsMatrix)

# Need to fix tame data
hist(samp$y)
samp[375,1] = 40
plot(samp$y,samp$x2)
plot(samp$y,samp$x1)
plot(samp$y,samp$x3)
fit_mod = lm(y ~ x1 + x2 + x3, data=samp)
resultsMatrix = Leave_one_out(samp, fit_mod)
tail(resultsMatrix)

```
From the results, it appears that the coeffeicents are not varying too much, which would suggest that the models are not overfitting the data. However, the differences in the predicted and actual values do vary slightly with each model prediction. Despite this varation, all the models constructed have relatively the same r squared value of roughtly 60%, which would suggest that all of the models have about the same predictive power.  